{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import talib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import quandl\n",
    "import datetime\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "py.init_notebook_mode(connected=True)\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Convolution2D,MaxPooling2D,Activation,Reshape,Permute\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.layers import LSTM,Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalprice=pd.read_csv('fianaldata.csv',index_col='date')\n",
    "finalprice.index=pd.to_datetime(finalprice.index,format=\"%Y-%m-%d\")\n",
    "finalprice=finalprice['2016-10-30':'2019-03-31']\n",
    "finalprice.replace(0,np.nan,inplace=True)\n",
    "finalprice=finalprice.fillna(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price= finalprice.loc['2018-04-01':'2019-03-31',['Close']]\n",
    "price=price.values\n",
    "trueprice=finalprice.loc['2018-04-01':'2019-03-31',['predict']]\n",
    "forecase=np.zeros((365,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2017-10-30':'2018-04-30']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-30)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-30-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-30),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(30,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(30,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(30,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(30,)\n",
    "\n",
    "for i in range(365-365,395-365): \n",
    "    forecase[i]=Yhat[i-365+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2017-11-29':'2018-05-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(395-365,426-365): \n",
    "    forecase[i]=Yhat[i-395+365]    \n",
    "forecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2017-12-30':'2018-06-30']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-30)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-30-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-30),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(30,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(30,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(30,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(30,)\n",
    "\n",
    "for i in range(426-365,456-365): \n",
    "    forecase[i]=Yhat[i-426+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-01-30':'2018-07-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(456-365,487-365): \n",
    "    forecase[i]=Yhat[i-456+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-02-27':'2018-08-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(487-365,518-365): \n",
    "    forecase[i]=Yhat[i-487+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-03-30':'2018-09-30']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-30)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-30-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-30),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(30,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(30,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(30,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(30,)\n",
    "\n",
    "for i in range(518-365,548-365): \n",
    "    forecase[i]=Yhat[i-518+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-04-29':'2018-10-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(548-365,579-365): \n",
    "    forecase[i]=Yhat[i-548+365]    \n",
    "forecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-05-30':'2018-11-30']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-30)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-30-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-30),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(30,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(30,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(30,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(30,)\n",
    "\n",
    "for i in range(579-365,609-365): \n",
    "    forecase[i]=Yhat[i-579+365]    \n",
    "forecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-06-29':'2018-12-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(609-365,640-365): \n",
    "    forecase[i]=Yhat[i-609+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-07-30':'2019-01-31']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(640-365,671-365): \n",
    "    forecase[i]=Yhat[i-640+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-08-30':'2019-02-28']\n",
    "\n",
    "data=final.values\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-28)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        #y=normalized_train_data[i:i+time_step,17,np.newaxis]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-28-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "   # size=(len(normalized_test_data)+time_step-1)//time_step\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        #x=normalized_test_data[i*time_step:(i+1)*time_step,:17]\n",
    "        #y=normalized_test_data[i*time_step:(i+1)*time_step,17]\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-28),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(28,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(28,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "#model.add(Dense(15))\n",
    "\n",
    "\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "#model.add(Permute(1,15))\n",
    "#model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "\n",
    "#model.add(LSTM(30, input_shape=(1, 50),return_sequences=False))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(LSTM(15,return_sequences=True))\n",
    "\n",
    "#model.add(LSTM(10,return_sequences=False))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(28,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(28,)\n",
    "\n",
    "for i in range(671-365,699-365): \n",
    "    forecase[i]=Yhat[i-671+365]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=finalprice['2018-09-29':'2019-03-31']\n",
    "\n",
    "data=final.values\n",
    "\n",
    "def get_train_data(batch_size=1,time_step=3,train_begin=0,train_end=(len(final)-31)):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    mean=np.mean(data_train,axis=0)\n",
    "    std=np.std(data_train,axis=0)\n",
    "    normalized_train_data=(data_train-mean)/std\n",
    "    size=(len(normalized_train_data)+time_step-1)//time_step\n",
    "    train_x,train_y=[],[]\n",
    "    for i in range(len(normalized_train_data)-time_step+1):\n",
    "        if i % batch_size==0:\n",
    "            batch_index.append(i)\n",
    "        x=normalized_train_data[i:i+time_step,:17]\n",
    "        y=normalized_train_data[i+time_step-1,17,np.newaxis]\n",
    "        train_x.append(x.tolist())\n",
    "        train_y.append(y.tolist())\n",
    "    return mean,std,train_x,train_y\n",
    "mean,std,train_x,train_y=get_train_data()\n",
    "def get_test_data(time_step=3,test_begin=(len(final)-31-2)):\n",
    "    data_test=data[test_begin:]\n",
    "    normalized_test_data=(data_test-mean)/std\n",
    "    size=(len(normalized_test_data)-time_step+1)\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size):\n",
    "        x=normalized_test_data[i:i+time_step,:17]\n",
    "        y=normalized_test_data[i+time_step-1,17,np.newaxis]\n",
    "        test_x.append(x.tolist())\n",
    "        test_y.append(y.tolist())\n",
    "    return test_x,test_y\n",
    "test_x,test_y=get_test_data()\n",
    "train_x=np.array(train_x)\n",
    "train_x=train_x.reshape(train_x.shape[0],3,17,1)\n",
    "train_y=np.array(train_y)\n",
    "train_y=train_y.reshape((len(final)-2-31),1)\n",
    "test_x=np.array(test_x)\n",
    "test_x=test_x.reshape(31,3,17,1)\n",
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(31,1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                1),\n",
    "   kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same'))\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "model.add(Convolution2D(\n",
    "    input_shape=(3,\n",
    "                 17,\n",
    "                 1),\n",
    "    kernel_size=(2,2),\n",
    "    filters=15,\n",
    "    padding='same'\n",
    "))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Reshape((1,25),input_shape=(25,)))\n",
    "model.add(LSTM(50,return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.add(Dropout(0.1))\n",
    "adam=Adam(lr=0.01)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(test_x, test_y), verbose=0,shuffle=False)\n",
    "    \n",
    "test_y=np.array(test_y)\n",
    "yhat = model.predict(test_x)\n",
    "Yhat=yhat*std[17]+mean[17]\n",
    "Test_y=test_y*std[17]+mean[17]\n",
    "Test_y=Test_y.reshape(31,)\n",
    "Yhat=np.array(Yhat)\n",
    "Yhat=Yhat.reshape(31,)\n",
    "\n",
    "for i in range(699-365,730-365): \n",
    "    forecase[i]=Yhat[i-699+365]    \n",
    "forecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "trueprice=trueprice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyplot.plot(forecase)\n",
    "pyplot.plot(trueprice)\n",
    "pyplot.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueprice=trueprice.reshape(365,)\n",
    "forecase=np.array(forecase)\n",
    "forecase=forecase.reshape(365,)\n",
    "error=abs(trueprice-forecase)\n",
    "error1=error/trueprice\n",
    "pyplot.plot(error1)\n",
    "pyplot.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error2=error*error \n",
    "rmse=pow(sum(error2)/365,0.5) \n",
    "mae=sum(error)/365\n",
    "mape=100*sum(error/trueprice)/365\n",
    "x=np.zeros((365,)) \n",
    "y=np.zeros((365,)) \n",
    "a=np.zeros((365,)) \n",
    "b=np.zeros((365,)) \n",
    "c=np.zeros((365,)) \n",
    "for i in range(365): \n",
    "    if trueprice[i]>=price[i]:\n",
    "        x[i]=1 \n",
    "    if forecase[i]>=price[i]: \n",
    "        y[i]=1 \n",
    "for i in range(365): \n",
    "    if x[i]==1 and y[i]==1: \n",
    "        a[i]=1 \n",
    "    if x[i]==1 and y[i]==0: \n",
    "        b[i]=1 \n",
    "    if x[i]==0 and y[i]==1: \n",
    "        c[i]=1 \n",
    "precision=sum(a)/(sum(a)+sum(b)) \n",
    "recall=sum(a)/(sum(a)+sum(c)) \n",
    "f1=2*precision*recall/(precision+recall) \n",
    "outcome=np.zeros((371,)) \n",
    "for i in range(365): \n",
    "    outcome[i]=forecase[i] \n",
    "outcome[365]=mae \n",
    "outcome[366]=rmse \n",
    "outcome[367]=mape\n",
    "outcome[368]=precision \n",
    "outcome[369]=recall \n",
    "outcome[370]=f1 \n",
    "outcome=pd.DataFrame(outcome) \n",
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outcome.to_csv('data20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueprice.to_csv('trueprice.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>7054.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-02</th>\n",
       "      <td>7414.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-03</th>\n",
       "      <td>6790.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-04</th>\n",
       "      <td>6763.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-05</th>\n",
       "      <td>6616.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-06</th>\n",
       "      <td>6893.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-07</th>\n",
       "      <td>7014.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-08</th>\n",
       "      <td>6773.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-09</th>\n",
       "      <td>6830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-10</th>\n",
       "      <td>6941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-11</th>\n",
       "      <td>7919.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-12</th>\n",
       "      <td>7882.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-13</th>\n",
       "      <td>8004.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-14</th>\n",
       "      <td>8363.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-15</th>\n",
       "      <td>8057.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-16</th>\n",
       "      <td>7893.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-17</th>\n",
       "      <td>8170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-18</th>\n",
       "      <td>8275.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-19</th>\n",
       "      <td>8875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-20</th>\n",
       "      <td>8944.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-21</th>\n",
       "      <td>8811.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-22</th>\n",
       "      <td>8954.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-23</th>\n",
       "      <td>9661.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-24</th>\n",
       "      <td>8855.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-25</th>\n",
       "      <td>9291.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-26</th>\n",
       "      <td>8917.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27</th>\n",
       "      <td>9339.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28</th>\n",
       "      <td>9411.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-29</th>\n",
       "      <td>9243.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>9081.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>3785.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>3700.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>3844.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>3851.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>3854.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>3844.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>3919.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>3897.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>3850.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>3863.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>3853.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>3853.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>3900.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>3989.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>3965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>3969.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>4032.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>3972.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>3982.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>3982.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>3969.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>3905.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>3918.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>4025.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>4013.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>4088.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>4090.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>4095.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>4137.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict\n",
       "date               \n",
       "2018-04-01   7054.7\n",
       "2018-04-02   7414.8\n",
       "2018-04-03   6790.4\n",
       "2018-04-04   6763.5\n",
       "2018-04-05   6616.2\n",
       "2018-04-06   6893.1\n",
       "2018-04-07   7014.5\n",
       "2018-04-08   6773.4\n",
       "2018-04-09   6830.0\n",
       "2018-04-10   6941.0\n",
       "2018-04-11   7919.9\n",
       "2018-04-12   7882.5\n",
       "2018-04-13   8004.9\n",
       "2018-04-14   8363.3\n",
       "2018-04-15   8057.1\n",
       "2018-04-16   7893.1\n",
       "2018-04-17   8170.0\n",
       "2018-04-18   8275.7\n",
       "2018-04-19   8875.0\n",
       "2018-04-20   8944.6\n",
       "2018-04-21   8811.8\n",
       "2018-04-22   8954.1\n",
       "2018-04-23   9661.7\n",
       "2018-04-24   8855.7\n",
       "2018-04-25   9291.4\n",
       "2018-04-26   8917.8\n",
       "2018-04-27   9339.2\n",
       "2018-04-28   9411.2\n",
       "2018-04-29   9243.2\n",
       "2018-04-30   9081.3\n",
       "...             ...\n",
       "2019-03-02   3785.7\n",
       "2019-03-03   3700.1\n",
       "2019-03-04   3844.9\n",
       "2019-03-05   3851.7\n",
       "2019-03-06   3854.0\n",
       "2019-03-07   3844.3\n",
       "2019-03-08   3919.2\n",
       "2019-03-09   3897.9\n",
       "2019-03-10   3850.7\n",
       "2019-03-11   3863.1\n",
       "2019-03-12   3853.4\n",
       "2019-03-13   3853.9\n",
       "2019-03-14   3900.2\n",
       "2019-03-15   3989.2\n",
       "2019-03-16   3965.0\n",
       "2019-03-17   3969.6\n",
       "2019-03-18   4000.0\n",
       "2019-03-19   4032.1\n",
       "2019-03-20   3972.4\n",
       "2019-03-21   3982.1\n",
       "2019-03-22   3982.5\n",
       "2019-03-23   3969.3\n",
       "2019-03-24   3905.9\n",
       "2019-03-25   3918.9\n",
       "2019-03-26   4025.8\n",
       "2019-03-27   4013.8\n",
       "2019-03-28   4088.9\n",
       "2019-03-29   4090.9\n",
       "2019-03-30   4095.2\n",
       "2019-03-31   4137.8\n",
       "\n",
       "[365 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trueprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume (BTC)</th>\n",
       "      <th>Volume (Currency)</th>\n",
       "      <th>index</th>\n",
       "      <th>biaopu</th>\n",
       "      <th>exchange</th>\n",
       "      <th>gold</th>\n",
       "      <th>nasdaq</th>\n",
       "      <th>niujiaosuo</th>\n",
       "      <th>oil</th>\n",
       "      <th>rate</th>\n",
       "      <th>RSI</th>\n",
       "      <th>mfi</th>\n",
       "      <th>obv</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-30</th>\n",
       "      <td>713.949</td>\n",
       "      <td>713.989</td>\n",
       "      <td>688.000</td>\n",
       "      <td>700.917</td>\n",
       "      <td>1750.780129</td>\n",
       "      <td>1.228539e+06</td>\n",
       "      <td>5565</td>\n",
       "      <td>2126.41</td>\n",
       "      <td>6.7858</td>\n",
       "      <td>1273.00</td>\n",
       "      <td>5190.10</td>\n",
       "      <td>10476.62</td>\n",
       "      <td>48.70</td>\n",
       "      <td>0.41</td>\n",
       "      <td>65.293669</td>\n",
       "      <td>6.197086e+01</td>\n",
       "      <td>59041.91285</td>\n",
       "      <td>698.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>700.079</td>\n",
       "      <td>708.899</td>\n",
       "      <td>685.700</td>\n",
       "      <td>698.919</td>\n",
       "      <td>982.631046</td>\n",
       "      <td>6.878437e+05</td>\n",
       "      <td>4467</td>\n",
       "      <td>2126.15</td>\n",
       "      <td>6.7641</td>\n",
       "      <td>1272.00</td>\n",
       "      <td>5189.14</td>\n",
       "      <td>10481.89</td>\n",
       "      <td>46.86</td>\n",
       "      <td>0.31</td>\n",
       "      <td>60.736373</td>\n",
       "      <td>3.790429e+01</td>\n",
       "      <td>58059.28180</td>\n",
       "      <td>733.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>698.790</td>\n",
       "      <td>739.488</td>\n",
       "      <td>697.910</td>\n",
       "      <td>733.744</td>\n",
       "      <td>2336.219977</td>\n",
       "      <td>1.702318e+06</td>\n",
       "      <td>4587</td>\n",
       "      <td>2111.72</td>\n",
       "      <td>6.7734</td>\n",
       "      <td>1288.45</td>\n",
       "      <td>5153.58</td>\n",
       "      <td>10414.05</td>\n",
       "      <td>46.67</td>\n",
       "      <td>0.41</td>\n",
       "      <td>86.100549</td>\n",
       "      <td>4.691699e+01</td>\n",
       "      <td>60395.50178</td>\n",
       "      <td>747.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-02</th>\n",
       "      <td>733.742</td>\n",
       "      <td>747.070</td>\n",
       "      <td>722.890</td>\n",
       "      <td>747.070</td>\n",
       "      <td>1902.393632</td>\n",
       "      <td>1.390995e+06</td>\n",
       "      <td>6337</td>\n",
       "      <td>2097.94</td>\n",
       "      <td>6.7562</td>\n",
       "      <td>1303.75</td>\n",
       "      <td>5105.57</td>\n",
       "      <td>10334.50</td>\n",
       "      <td>45.34</td>\n",
       "      <td>0.41</td>\n",
       "      <td>89.860276</td>\n",
       "      <td>8.187062e+01</td>\n",
       "      <td>62297.89541</td>\n",
       "      <td>692.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-03</th>\n",
       "      <td>747.070</td>\n",
       "      <td>750.500</td>\n",
       "      <td>673.407</td>\n",
       "      <td>692.179</td>\n",
       "      <td>4169.139506</td>\n",
       "      <td>2.969056e+06</td>\n",
       "      <td>6845</td>\n",
       "      <td>2088.66</td>\n",
       "      <td>6.7491</td>\n",
       "      <td>1301.00</td>\n",
       "      <td>5058.41</td>\n",
       "      <td>10307.64</td>\n",
       "      <td>44.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>33.639305</td>\n",
       "      <td>5.129104e+01</td>\n",
       "      <td>58128.75590</td>\n",
       "      <td>706.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-04</th>\n",
       "      <td>692.902</td>\n",
       "      <td>709.998</td>\n",
       "      <td>682.311</td>\n",
       "      <td>706.036</td>\n",
       "      <td>1904.910013</td>\n",
       "      <td>1.335331e+06</td>\n",
       "      <td>6811</td>\n",
       "      <td>2085.18</td>\n",
       "      <td>6.7514</td>\n",
       "      <td>1302.80</td>\n",
       "      <td>5046.37</td>\n",
       "      <td>10289.35</td>\n",
       "      <td>44.07</td>\n",
       "      <td>0.41</td>\n",
       "      <td>46.349772</td>\n",
       "      <td>2.475579e+01</td>\n",
       "      <td>60033.66592</td>\n",
       "      <td>706.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-05</th>\n",
       "      <td>706.036</td>\n",
       "      <td>714.471</td>\n",
       "      <td>698.478</td>\n",
       "      <td>706.478</td>\n",
       "      <td>630.687729</td>\n",
       "      <td>4.452695e+05</td>\n",
       "      <td>6048</td>\n",
       "      <td>2085.18</td>\n",
       "      <td>6.7514</td>\n",
       "      <td>1302.80</td>\n",
       "      <td>5046.37</td>\n",
       "      <td>10289.35</td>\n",
       "      <td>44.07</td>\n",
       "      <td>0.41</td>\n",
       "      <td>46.836969</td>\n",
       "      <td>9.442540e+00</td>\n",
       "      <td>60664.35365</td>\n",
       "      <td>715.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-06</th>\n",
       "      <td>706.460</td>\n",
       "      <td>721.458</td>\n",
       "      <td>702.879</td>\n",
       "      <td>715.497</td>\n",
       "      <td>638.744099</td>\n",
       "      <td>4.555510e+05</td>\n",
       "      <td>4572</td>\n",
       "      <td>2085.18</td>\n",
       "      <td>6.7514</td>\n",
       "      <td>1302.80</td>\n",
       "      <td>5046.37</td>\n",
       "      <td>10289.35</td>\n",
       "      <td>44.07</td>\n",
       "      <td>0.41</td>\n",
       "      <td>58.399626</td>\n",
       "      <td>4.034681e+01</td>\n",
       "      <td>61303.09775</td>\n",
       "      <td>707.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-07</th>\n",
       "      <td>715.000</td>\n",
       "      <td>715.413</td>\n",
       "      <td>700.160</td>\n",
       "      <td>707.345</td>\n",
       "      <td>917.369981</td>\n",
       "      <td>6.484827e+05</td>\n",
       "      <td>5370</td>\n",
       "      <td>2131.52</td>\n",
       "      <td>6.7725</td>\n",
       "      <td>1283.05</td>\n",
       "      <td>5166.17</td>\n",
       "      <td>10500.16</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.41</td>\n",
       "      <td>45.100424</td>\n",
       "      <td>5.812730e+01</td>\n",
       "      <td>60385.72776</td>\n",
       "      <td>712.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-08</th>\n",
       "      <td>704.142</td>\n",
       "      <td>718.300</td>\n",
       "      <td>704.142</td>\n",
       "      <td>712.500</td>\n",
       "      <td>967.349179</td>\n",
       "      <td>6.879393e+05</td>\n",
       "      <td>5380</td>\n",
       "      <td>2139.56</td>\n",
       "      <td>6.7817</td>\n",
       "      <td>1282.35</td>\n",
       "      <td>5193.49</td>\n",
       "      <td>10530.56</td>\n",
       "      <td>44.98</td>\n",
       "      <td>0.41</td>\n",
       "      <td>54.852646</td>\n",
       "      <td>6.379801e+01</td>\n",
       "      <td>61353.07694</td>\n",
       "      <td>722.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09</th>\n",
       "      <td>712.690</td>\n",
       "      <td>745.492</td>\n",
       "      <td>708.682</td>\n",
       "      <td>722.400</td>\n",
       "      <td>2759.317961</td>\n",
       "      <td>2.012925e+06</td>\n",
       "      <td>5240</td>\n",
       "      <td>2163.26</td>\n",
       "      <td>6.7832</td>\n",
       "      <td>1281.40</td>\n",
       "      <td>5251.07</td>\n",
       "      <td>10643.41</td>\n",
       "      <td>45.27</td>\n",
       "      <td>0.41</td>\n",
       "      <td>70.135108</td>\n",
       "      <td>8.056113e+01</td>\n",
       "      <td>64112.39490</td>\n",
       "      <td>714.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-10</th>\n",
       "      <td>722.400</td>\n",
       "      <td>724.696</td>\n",
       "      <td>706.000</td>\n",
       "      <td>714.000</td>\n",
       "      <td>1231.380014</td>\n",
       "      <td>8.802056e+05</td>\n",
       "      <td>4749</td>\n",
       "      <td>2167.48</td>\n",
       "      <td>6.7885</td>\n",
       "      <td>1267.50</td>\n",
       "      <td>5208.80</td>\n",
       "      <td>10683.41</td>\n",
       "      <td>44.66</td>\n",
       "      <td>0.41</td>\n",
       "      <td>49.017405</td>\n",
       "      <td>7.534608e+01</td>\n",
       "      <td>62881.01489</td>\n",
       "      <td>717.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-11</th>\n",
       "      <td>713.503</td>\n",
       "      <td>718.990</td>\n",
       "      <td>712.002</td>\n",
       "      <td>717.750</td>\n",
       "      <td>719.858395</td>\n",
       "      <td>5.152334e+05</td>\n",
       "      <td>3735</td>\n",
       "      <td>2164.45</td>\n",
       "      <td>6.8115</td>\n",
       "      <td>1236.45</td>\n",
       "      <td>5237.11</td>\n",
       "      <td>10652.24</td>\n",
       "      <td>44.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>57.572124</td>\n",
       "      <td>7.409218e+01</td>\n",
       "      <td>63600.87328</td>\n",
       "      <td>704.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-12</th>\n",
       "      <td>717.740</td>\n",
       "      <td>719.100</td>\n",
       "      <td>701.348</td>\n",
       "      <td>704.000</td>\n",
       "      <td>722.713797</td>\n",
       "      <td>5.113135e+05</td>\n",
       "      <td>4153</td>\n",
       "      <td>2164.45</td>\n",
       "      <td>6.8115</td>\n",
       "      <td>1236.45</td>\n",
       "      <td>5237.11</td>\n",
       "      <td>10652.24</td>\n",
       "      <td>44.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>29.940528</td>\n",
       "      <td>2.702717e+01</td>\n",
       "      <td>62878.15949</td>\n",
       "      <td>701.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13</th>\n",
       "      <td>704.399</td>\n",
       "      <td>706.794</td>\n",
       "      <td>685.327</td>\n",
       "      <td>701.670</td>\n",
       "      <td>1072.468637</td>\n",
       "      <td>7.445580e+05</td>\n",
       "      <td>4225</td>\n",
       "      <td>2164.45</td>\n",
       "      <td>6.8115</td>\n",
       "      <td>1236.45</td>\n",
       "      <td>5237.11</td>\n",
       "      <td>10652.24</td>\n",
       "      <td>44.15</td>\n",
       "      <td>0.41</td>\n",
       "      <td>26.685107</td>\n",
       "      <td>2.903307e+01</td>\n",
       "      <td>61805.69085</td>\n",
       "      <td>706.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-14</th>\n",
       "      <td>702.700</td>\n",
       "      <td>709.000</td>\n",
       "      <td>697.791</td>\n",
       "      <td>706.994</td>\n",
       "      <td>785.108494</td>\n",
       "      <td>5.512836e+05</td>\n",
       "      <td>7171</td>\n",
       "      <td>2164.20</td>\n",
       "      <td>6.8291</td>\n",
       "      <td>1213.60</td>\n",
       "      <td>5218.40</td>\n",
       "      <td>10679.77</td>\n",
       "      <td>43.94</td>\n",
       "      <td>0.41</td>\n",
       "      <td>46.589460</td>\n",
       "      <td>3.050396e+01</td>\n",
       "      <td>62590.79934</td>\n",
       "      <td>711.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-15</th>\n",
       "      <td>706.999</td>\n",
       "      <td>717.000</td>\n",
       "      <td>706.081</td>\n",
       "      <td>711.310</td>\n",
       "      <td>848.937713</td>\n",
       "      <td>6.048656e+05</td>\n",
       "      <td>5907</td>\n",
       "      <td>2180.39</td>\n",
       "      <td>6.8495</td>\n",
       "      <td>1226.95</td>\n",
       "      <td>5275.62</td>\n",
       "      <td>10745.51</td>\n",
       "      <td>46.39</td>\n",
       "      <td>0.41</td>\n",
       "      <td>59.845750</td>\n",
       "      <td>6.072225e+01</td>\n",
       "      <td>63439.73706</td>\n",
       "      <td>741.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-16</th>\n",
       "      <td>710.870</td>\n",
       "      <td>744.800</td>\n",
       "      <td>709.000</td>\n",
       "      <td>741.603</td>\n",
       "      <td>2465.339553</td>\n",
       "      <td>1.804220e+06</td>\n",
       "      <td>5995</td>\n",
       "      <td>2176.94</td>\n",
       "      <td>6.8592</td>\n",
       "      <td>1229.20</td>\n",
       "      <td>5294.58</td>\n",
       "      <td>10699.43</td>\n",
       "      <td>46.10</td>\n",
       "      <td>0.41</td>\n",
       "      <td>88.886318</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>65905.07661</td>\n",
       "      <td>739.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-17</th>\n",
       "      <td>741.875</td>\n",
       "      <td>760.814</td>\n",
       "      <td>733.300</td>\n",
       "      <td>739.100</td>\n",
       "      <td>3453.915169</td>\n",
       "      <td>2.576195e+06</td>\n",
       "      <td>7811</td>\n",
       "      <td>2187.12</td>\n",
       "      <td>6.8692</td>\n",
       "      <td>1226.75</td>\n",
       "      <td>5333.97</td>\n",
       "      <td>10740.08</td>\n",
       "      <td>45.98</td>\n",
       "      <td>0.41</td>\n",
       "      <td>81.574308</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>62451.16144</td>\n",
       "      <td>752.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-18</th>\n",
       "      <td>739.088</td>\n",
       "      <td>754.000</td>\n",
       "      <td>733.010</td>\n",
       "      <td>752.232</td>\n",
       "      <td>1730.442485</td>\n",
       "      <td>1.292808e+06</td>\n",
       "      <td>7623</td>\n",
       "      <td>2181.90</td>\n",
       "      <td>6.8796</td>\n",
       "      <td>1211.00</td>\n",
       "      <td>5321.51</td>\n",
       "      <td>10709.51</td>\n",
       "      <td>46.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>88.815193</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>64181.60393</td>\n",
       "      <td>751.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-19</th>\n",
       "      <td>752.232</td>\n",
       "      <td>763.110</td>\n",
       "      <td>735.000</td>\n",
       "      <td>751.990</td>\n",
       "      <td>1420.026691</td>\n",
       "      <td>1.063850e+06</td>\n",
       "      <td>7221</td>\n",
       "      <td>2181.90</td>\n",
       "      <td>6.8796</td>\n",
       "      <td>1211.00</td>\n",
       "      <td>5321.51</td>\n",
       "      <td>10709.51</td>\n",
       "      <td>46.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>87.860775</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>62761.57724</td>\n",
       "      <td>729.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-20</th>\n",
       "      <td>751.990</td>\n",
       "      <td>757.000</td>\n",
       "      <td>713.000</td>\n",
       "      <td>729.677</td>\n",
       "      <td>1413.783356</td>\n",
       "      <td>1.040871e+06</td>\n",
       "      <td>5976</td>\n",
       "      <td>2181.90</td>\n",
       "      <td>6.8796</td>\n",
       "      <td>1211.00</td>\n",
       "      <td>5321.51</td>\n",
       "      <td>10709.51</td>\n",
       "      <td>46.36</td>\n",
       "      <td>0.41</td>\n",
       "      <td>35.338996</td>\n",
       "      <td>6.945106e+01</td>\n",
       "      <td>61347.79388</td>\n",
       "      <td>738.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-21</th>\n",
       "      <td>731.000</td>\n",
       "      <td>741.000</td>\n",
       "      <td>728.330</td>\n",
       "      <td>738.000</td>\n",
       "      <td>545.090343</td>\n",
       "      <td>4.011288e+05</td>\n",
       "      <td>5694</td>\n",
       "      <td>2198.18</td>\n",
       "      <td>6.8985</td>\n",
       "      <td>1214.25</td>\n",
       "      <td>5368.86</td>\n",
       "      <td>10791.84</td>\n",
       "      <td>48.24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>51.545566</td>\n",
       "      <td>5.858074e+01</td>\n",
       "      <td>61892.88422</td>\n",
       "      <td>749.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-22</th>\n",
       "      <td>737.999</td>\n",
       "      <td>752.614</td>\n",
       "      <td>733.830</td>\n",
       "      <td>749.760</td>\n",
       "      <td>1806.774640</td>\n",
       "      <td>1.344081e+06</td>\n",
       "      <td>5312</td>\n",
       "      <td>2202.94</td>\n",
       "      <td>6.8779</td>\n",
       "      <td>1212.25</td>\n",
       "      <td>5386.35</td>\n",
       "      <td>10820.18</td>\n",
       "      <td>48.03</td>\n",
       "      <td>0.41</td>\n",
       "      <td>68.355489</td>\n",
       "      <td>6.277115e+01</td>\n",
       "      <td>63699.65886</td>\n",
       "      <td>741.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-23</th>\n",
       "      <td>749.760</td>\n",
       "      <td>750.000</td>\n",
       "      <td>731.060</td>\n",
       "      <td>741.980</td>\n",
       "      <td>1283.534229</td>\n",
       "      <td>9.510890e+05</td>\n",
       "      <td>5380</td>\n",
       "      <td>2204.72</td>\n",
       "      <td>6.8904</td>\n",
       "      <td>1185.35</td>\n",
       "      <td>5380.69</td>\n",
       "      <td>10835.90</td>\n",
       "      <td>47.96</td>\n",
       "      <td>0.41</td>\n",
       "      <td>50.849627</td>\n",
       "      <td>6.475982e+01</td>\n",
       "      <td>62416.12463</td>\n",
       "      <td>738.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-24</th>\n",
       "      <td>742.489</td>\n",
       "      <td>745.110</td>\n",
       "      <td>730.000</td>\n",
       "      <td>738.440</td>\n",
       "      <td>867.198395</td>\n",
       "      <td>6.390440e+05</td>\n",
       "      <td>6227</td>\n",
       "      <td>2204.72</td>\n",
       "      <td>6.9085</td>\n",
       "      <td>1186.10</td>\n",
       "      <td>5380.69</td>\n",
       "      <td>10835.90</td>\n",
       "      <td>47.96</td>\n",
       "      <td>0.41</td>\n",
       "      <td>43.283888</td>\n",
       "      <td>4.584366e+01</td>\n",
       "      <td>61548.92624</td>\n",
       "      <td>742.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-25</th>\n",
       "      <td>740.459</td>\n",
       "      <td>743.000</td>\n",
       "      <td>730.345</td>\n",
       "      <td>742.140</td>\n",
       "      <td>985.011973</td>\n",
       "      <td>7.267828e+05</td>\n",
       "      <td>5634</td>\n",
       "      <td>2213.35</td>\n",
       "      <td>6.9168</td>\n",
       "      <td>1187.70</td>\n",
       "      <td>5398.92</td>\n",
       "      <td>10878.09</td>\n",
       "      <td>46.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>54.011489</td>\n",
       "      <td>3.137616e+01</td>\n",
       "      <td>62533.93821</td>\n",
       "      <td>733.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-26</th>\n",
       "      <td>742.479</td>\n",
       "      <td>743.300</td>\n",
       "      <td>726.000</td>\n",
       "      <td>733.170</td>\n",
       "      <td>678.236092</td>\n",
       "      <td>4.977535e+05</td>\n",
       "      <td>4480</td>\n",
       "      <td>2213.35</td>\n",
       "      <td>6.9168</td>\n",
       "      <td>1187.70</td>\n",
       "      <td>5398.92</td>\n",
       "      <td>10878.09</td>\n",
       "      <td>46.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>32.000640</td>\n",
       "      <td>3.899949e+01</td>\n",
       "      <td>61855.70212</td>\n",
       "      <td>729.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-27</th>\n",
       "      <td>733.260</td>\n",
       "      <td>739.000</td>\n",
       "      <td>727.200</td>\n",
       "      <td>729.930</td>\n",
       "      <td>637.789927</td>\n",
       "      <td>4.673402e+05</td>\n",
       "      <td>4324</td>\n",
       "      <td>2213.35</td>\n",
       "      <td>6.9168</td>\n",
       "      <td>1187.70</td>\n",
       "      <td>5398.92</td>\n",
       "      <td>10878.09</td>\n",
       "      <td>46.06</td>\n",
       "      <td>0.41</td>\n",
       "      <td>26.212894</td>\n",
       "      <td>4.298581e+01</td>\n",
       "      <td>61217.91219</td>\n",
       "      <td>734.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-28</th>\n",
       "      <td>729.885</td>\n",
       "      <td>736.071</td>\n",
       "      <td>728.100</td>\n",
       "      <td>734.170</td>\n",
       "      <td>819.240542</td>\n",
       "      <td>6.005968e+05</td>\n",
       "      <td>3797</td>\n",
       "      <td>2201.72</td>\n",
       "      <td>6.9042</td>\n",
       "      <td>1187.00</td>\n",
       "      <td>5368.81</td>\n",
       "      <td>10808.63</td>\n",
       "      <td>47.08</td>\n",
       "      <td>0.41</td>\n",
       "      <td>45.545706</td>\n",
       "      <td>3.835577e+01</td>\n",
       "      <td>62037.15273</td>\n",
       "      <td>732.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-02</th>\n",
       "      <td>3804.600</td>\n",
       "      <td>3818.500</td>\n",
       "      <td>3754.300</td>\n",
       "      <td>3809.900</td>\n",
       "      <td>1766.652670</td>\n",
       "      <td>6.714834e+06</td>\n",
       "      <td>11473</td>\n",
       "      <td>2803.69</td>\n",
       "      <td>6.6957</td>\n",
       "      <td>1311.95</td>\n",
       "      <td>7595.35</td>\n",
       "      <td>12700.67</td>\n",
       "      <td>55.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>48.986830</td>\n",
       "      <td>5.596731e+01</td>\n",
       "      <td>133771.85570</td>\n",
       "      <td>3785.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>3810.300</td>\n",
       "      <td>3822.400</td>\n",
       "      <td>3755.400</td>\n",
       "      <td>3785.700</td>\n",
       "      <td>2204.155819</td>\n",
       "      <td>8.362644e+06</td>\n",
       "      <td>11604</td>\n",
       "      <td>2803.69</td>\n",
       "      <td>6.6957</td>\n",
       "      <td>1311.95</td>\n",
       "      <td>7595.35</td>\n",
       "      <td>12700.67</td>\n",
       "      <td>55.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>33.935844</td>\n",
       "      <td>1.180000e-13</td>\n",
       "      <td>131567.69990</td>\n",
       "      <td>3700.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>3785.700</td>\n",
       "      <td>3805.200</td>\n",
       "      <td>3670.900</td>\n",
       "      <td>3700.100</td>\n",
       "      <td>4484.164154</td>\n",
       "      <td>1.665976e+07</td>\n",
       "      <td>13315</td>\n",
       "      <td>2792.81</td>\n",
       "      <td>6.7049</td>\n",
       "      <td>1285.40</td>\n",
       "      <td>7577.57</td>\n",
       "      <td>12637.27</td>\n",
       "      <td>56.59</td>\n",
       "      <td>2.40</td>\n",
       "      <td>12.902485</td>\n",
       "      <td>9.380000e-14</td>\n",
       "      <td>127083.53570</td>\n",
       "      <td>3844.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>3700.000</td>\n",
       "      <td>3875.100</td>\n",
       "      <td>3690.600</td>\n",
       "      <td>3844.900</td>\n",
       "      <td>4451.450719</td>\n",
       "      <td>1.690483e+07</td>\n",
       "      <td>13795</td>\n",
       "      <td>2789.65</td>\n",
       "      <td>6.6998</td>\n",
       "      <td>1283.80</td>\n",
       "      <td>7576.36</td>\n",
       "      <td>12624.47</td>\n",
       "      <td>56.56</td>\n",
       "      <td>2.40</td>\n",
       "      <td>66.145014</td>\n",
       "      <td>4.032638e+01</td>\n",
       "      <td>131534.98640</td>\n",
       "      <td>3851.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>3845.000</td>\n",
       "      <td>3895.700</td>\n",
       "      <td>3805.000</td>\n",
       "      <td>3851.700</td>\n",
       "      <td>3444.455415</td>\n",
       "      <td>1.323957e+07</td>\n",
       "      <td>11580</td>\n",
       "      <td>2771.45</td>\n",
       "      <td>6.7053</td>\n",
       "      <td>1285.85</td>\n",
       "      <td>7505.92</td>\n",
       "      <td>12538.00</td>\n",
       "      <td>56.22</td>\n",
       "      <td>2.40</td>\n",
       "      <td>67.542661</td>\n",
       "      <td>6.438136e+01</td>\n",
       "      <td>134979.44190</td>\n",
       "      <td>3854.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>3851.700</td>\n",
       "      <td>3891.400</td>\n",
       "      <td>3829.900</td>\n",
       "      <td>3854.000</td>\n",
       "      <td>4079.085996</td>\n",
       "      <td>1.575452e+07</td>\n",
       "      <td>12374</td>\n",
       "      <td>2748.93</td>\n",
       "      <td>6.7110</td>\n",
       "      <td>1285.30</td>\n",
       "      <td>7421.46</td>\n",
       "      <td>12443.43</td>\n",
       "      <td>56.66</td>\n",
       "      <td>2.40</td>\n",
       "      <td>68.208540</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>139058.52790</td>\n",
       "      <td>3844.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>3854.000</td>\n",
       "      <td>3910.000</td>\n",
       "      <td>3760.300</td>\n",
       "      <td>3844.300</td>\n",
       "      <td>4658.364326</td>\n",
       "      <td>1.799671e+07</td>\n",
       "      <td>10530</td>\n",
       "      <td>2743.07</td>\n",
       "      <td>6.7235</td>\n",
       "      <td>1296.75</td>\n",
       "      <td>7408.14</td>\n",
       "      <td>12415.13</td>\n",
       "      <td>56.07</td>\n",
       "      <td>2.40</td>\n",
       "      <td>60.373135</td>\n",
       "      <td>6.186269e+01</td>\n",
       "      <td>134400.16350</td>\n",
       "      <td>3919.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>3840.500</td>\n",
       "      <td>3947.000</td>\n",
       "      <td>3836.700</td>\n",
       "      <td>3919.200</td>\n",
       "      <td>3290.699350</td>\n",
       "      <td>1.281770e+07</td>\n",
       "      <td>12150</td>\n",
       "      <td>2743.07</td>\n",
       "      <td>6.7235</td>\n",
       "      <td>1296.75</td>\n",
       "      <td>7408.14</td>\n",
       "      <td>12415.13</td>\n",
       "      <td>56.07</td>\n",
       "      <td>2.40</td>\n",
       "      <td>82.996615</td>\n",
       "      <td>6.151215e+01</td>\n",
       "      <td>137690.86290</td>\n",
       "      <td>3897.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-10</th>\n",
       "      <td>3916.100</td>\n",
       "      <td>3916.300</td>\n",
       "      <td>3863.700</td>\n",
       "      <td>3897.900</td>\n",
       "      <td>1449.901130</td>\n",
       "      <td>5.639155e+06</td>\n",
       "      <td>11698</td>\n",
       "      <td>2743.07</td>\n",
       "      <td>6.7235</td>\n",
       "      <td>1296.75</td>\n",
       "      <td>7408.14</td>\n",
       "      <td>12415.13</td>\n",
       "      <td>56.07</td>\n",
       "      <td>2.40</td>\n",
       "      <td>66.742564</td>\n",
       "      <td>3.530447e+01</td>\n",
       "      <td>136240.96180</td>\n",
       "      <td>3850.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-11</th>\n",
       "      <td>3899.800</td>\n",
       "      <td>3911.200</td>\n",
       "      <td>3815.500</td>\n",
       "      <td>3850.700</td>\n",
       "      <td>3229.684251</td>\n",
       "      <td>1.245815e+07</td>\n",
       "      <td>12708</td>\n",
       "      <td>2783.30</td>\n",
       "      <td>6.7202</td>\n",
       "      <td>1292.75</td>\n",
       "      <td>7558.06</td>\n",
       "      <td>12561.26</td>\n",
       "      <td>56.79</td>\n",
       "      <td>2.40</td>\n",
       "      <td>40.426496</td>\n",
       "      <td>4.148348e+01</td>\n",
       "      <td>133011.27750</td>\n",
       "      <td>3863.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>3852.000</td>\n",
       "      <td>3879.600</td>\n",
       "      <td>3800.400</td>\n",
       "      <td>3863.100</td>\n",
       "      <td>3277.170766</td>\n",
       "      <td>1.261635e+07</td>\n",
       "      <td>12593</td>\n",
       "      <td>2791.52</td>\n",
       "      <td>6.7128</td>\n",
       "      <td>1297.05</td>\n",
       "      <td>7591.03</td>\n",
       "      <td>12582.81</td>\n",
       "      <td>57.20</td>\n",
       "      <td>2.40</td>\n",
       "      <td>48.438076</td>\n",
       "      <td>1.090000e-13</td>\n",
       "      <td>136288.44830</td>\n",
       "      <td>3853.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-13</th>\n",
       "      <td>3864.400</td>\n",
       "      <td>3873.500</td>\n",
       "      <td>3826.200</td>\n",
       "      <td>3853.400</td>\n",
       "      <td>3013.369886</td>\n",
       "      <td>1.161036e+07</td>\n",
       "      <td>12713</td>\n",
       "      <td>2810.92</td>\n",
       "      <td>6.7114</td>\n",
       "      <td>1306.95</td>\n",
       "      <td>7643.41</td>\n",
       "      <td>12674.26</td>\n",
       "      <td>58.59</td>\n",
       "      <td>2.40</td>\n",
       "      <td>41.836316</td>\n",
       "      <td>3.163915e+01</td>\n",
       "      <td>133275.07840</td>\n",
       "      <td>3853.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>3851.300</td>\n",
       "      <td>3907.000</td>\n",
       "      <td>3781.600</td>\n",
       "      <td>3853.900</td>\n",
       "      <td>3598.009049</td>\n",
       "      <td>1.384985e+07</td>\n",
       "      <td>12453</td>\n",
       "      <td>2808.48</td>\n",
       "      <td>6.7009</td>\n",
       "      <td>1295.55</td>\n",
       "      <td>7630.91</td>\n",
       "      <td>12660.16</td>\n",
       "      <td>58.91</td>\n",
       "      <td>2.40</td>\n",
       "      <td>42.442859</td>\n",
       "      <td>3.049225e+01</td>\n",
       "      <td>136873.08740</td>\n",
       "      <td>3900.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>3853.900</td>\n",
       "      <td>3909.800</td>\n",
       "      <td>3846.800</td>\n",
       "      <td>3900.200</td>\n",
       "      <td>2890.546813</td>\n",
       "      <td>1.122124e+07</td>\n",
       "      <td>11742</td>\n",
       "      <td>2822.48</td>\n",
       "      <td>6.7167</td>\n",
       "      <td>1303.50</td>\n",
       "      <td>7688.53</td>\n",
       "      <td>12715.77</td>\n",
       "      <td>58.82</td>\n",
       "      <td>2.40</td>\n",
       "      <td>76.492688</td>\n",
       "      <td>6.225858e+01</td>\n",
       "      <td>139763.63420</td>\n",
       "      <td>3989.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-16</th>\n",
       "      <td>3900.100</td>\n",
       "      <td>4042.500</td>\n",
       "      <td>3899.700</td>\n",
       "      <td>3989.200</td>\n",
       "      <td>4433.965270</td>\n",
       "      <td>1.763293e+07</td>\n",
       "      <td>10455</td>\n",
       "      <td>2822.48</td>\n",
       "      <td>6.7167</td>\n",
       "      <td>1303.50</td>\n",
       "      <td>7688.53</td>\n",
       "      <td>12715.77</td>\n",
       "      <td>58.82</td>\n",
       "      <td>2.40</td>\n",
       "      <td>91.312098</td>\n",
       "      <td>6.758707e+01</td>\n",
       "      <td>144197.59950</td>\n",
       "      <td>3965.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-17</th>\n",
       "      <td>3989.500</td>\n",
       "      <td>3989.600</td>\n",
       "      <td>3937.400</td>\n",
       "      <td>3965.000</td>\n",
       "      <td>1470.329979</td>\n",
       "      <td>5.825342e+06</td>\n",
       "      <td>10076</td>\n",
       "      <td>2822.48</td>\n",
       "      <td>6.7167</td>\n",
       "      <td>1303.50</td>\n",
       "      <td>7688.53</td>\n",
       "      <td>12715.77</td>\n",
       "      <td>58.82</td>\n",
       "      <td>2.40</td>\n",
       "      <td>72.635646</td>\n",
       "      <td>8.320077e+01</td>\n",
       "      <td>142727.26950</td>\n",
       "      <td>3969.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>3965.100</td>\n",
       "      <td>4019.900</td>\n",
       "      <td>3935.200</td>\n",
       "      <td>3969.600</td>\n",
       "      <td>3731.057533</td>\n",
       "      <td>1.481929e+07</td>\n",
       "      <td>8644</td>\n",
       "      <td>2832.94</td>\n",
       "      <td>6.7088</td>\n",
       "      <td>1305.60</td>\n",
       "      <td>7714.48</td>\n",
       "      <td>12781.42</td>\n",
       "      <td>59.38</td>\n",
       "      <td>2.40</td>\n",
       "      <td>74.143534</td>\n",
       "      <td>8.477967e+01</td>\n",
       "      <td>146458.32710</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>3969.600</td>\n",
       "      <td>4010.000</td>\n",
       "      <td>3950.000</td>\n",
       "      <td>4000.000</td>\n",
       "      <td>3209.596846</td>\n",
       "      <td>1.278029e+07</td>\n",
       "      <td>11494</td>\n",
       "      <td>2832.57</td>\n",
       "      <td>6.7062</td>\n",
       "      <td>1307.70</td>\n",
       "      <td>7723.95</td>\n",
       "      <td>12760.79</td>\n",
       "      <td>59.29</td>\n",
       "      <td>2.40</td>\n",
       "      <td>83.277943</td>\n",
       "      <td>8.257820e+01</td>\n",
       "      <td>149667.92390</td>\n",
       "      <td>4032.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>3999.900</td>\n",
       "      <td>4048.800</td>\n",
       "      <td>3963.400</td>\n",
       "      <td>4032.100</td>\n",
       "      <td>3934.607074</td>\n",
       "      <td>1.574005e+07</td>\n",
       "      <td>13154</td>\n",
       "      <td>2824.23</td>\n",
       "      <td>6.7101</td>\n",
       "      <td>1303.70</td>\n",
       "      <td>7728.97</td>\n",
       "      <td>12700.25</td>\n",
       "      <td>60.23</td>\n",
       "      <td>2.41</td>\n",
       "      <td>89.277596</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>153602.53100</td>\n",
       "      <td>3972.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>4030.900</td>\n",
       "      <td>4064.400</td>\n",
       "      <td>3871.300</td>\n",
       "      <td>3972.400</td>\n",
       "      <td>4867.144416</td>\n",
       "      <td>1.941989e+07</td>\n",
       "      <td>12650</td>\n",
       "      <td>2854.88</td>\n",
       "      <td>6.6850</td>\n",
       "      <td>1309.60</td>\n",
       "      <td>7838.96</td>\n",
       "      <td>12782.54</td>\n",
       "      <td>59.98</td>\n",
       "      <td>2.41</td>\n",
       "      <td>44.618395</td>\n",
       "      <td>5.967682e+01</td>\n",
       "      <td>148735.38660</td>\n",
       "      <td>3982.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>3972.500</td>\n",
       "      <td>3999.000</td>\n",
       "      <td>3960.000</td>\n",
       "      <td>3982.100</td>\n",
       "      <td>2481.959080</td>\n",
       "      <td>9.879739e+06</td>\n",
       "      <td>13195</td>\n",
       "      <td>2800.71</td>\n",
       "      <td>6.6944</td>\n",
       "      <td>1311.30</td>\n",
       "      <td>7642.67</td>\n",
       "      <td>12539.41</td>\n",
       "      <td>59.04</td>\n",
       "      <td>2.41</td>\n",
       "      <td>50.636541</td>\n",
       "      <td>5.706316e+01</td>\n",
       "      <td>151217.34570</td>\n",
       "      <td>3982.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-23</th>\n",
       "      <td>3982.100</td>\n",
       "      <td>4000.400</td>\n",
       "      <td>3961.000</td>\n",
       "      <td>3982.500</td>\n",
       "      <td>1616.990068</td>\n",
       "      <td>6.436828e+06</td>\n",
       "      <td>14116</td>\n",
       "      <td>2800.71</td>\n",
       "      <td>6.6944</td>\n",
       "      <td>1311.30</td>\n",
       "      <td>7642.67</td>\n",
       "      <td>12539.41</td>\n",
       "      <td>59.04</td>\n",
       "      <td>2.41</td>\n",
       "      <td>50.966130</td>\n",
       "      <td>4.578709e+01</td>\n",
       "      <td>152834.33570</td>\n",
       "      <td>3969.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-24</th>\n",
       "      <td>3982.400</td>\n",
       "      <td>3982.400</td>\n",
       "      <td>3944.400</td>\n",
       "      <td>3969.300</td>\n",
       "      <td>2067.316000</td>\n",
       "      <td>8.197462e+06</td>\n",
       "      <td>9716</td>\n",
       "      <td>2800.71</td>\n",
       "      <td>6.6944</td>\n",
       "      <td>1311.30</td>\n",
       "      <td>7642.67</td>\n",
       "      <td>12539.41</td>\n",
       "      <td>59.04</td>\n",
       "      <td>2.41</td>\n",
       "      <td>38.305976</td>\n",
       "      <td>6.655992e+01</td>\n",
       "      <td>150767.01970</td>\n",
       "      <td>3905.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>3968.600</td>\n",
       "      <td>3976.600</td>\n",
       "      <td>3855.500</td>\n",
       "      <td>3905.900</td>\n",
       "      <td>4822.948801</td>\n",
       "      <td>1.892225e+07</td>\n",
       "      <td>9152</td>\n",
       "      <td>2798.36</td>\n",
       "      <td>6.7098</td>\n",
       "      <td>1319.55</td>\n",
       "      <td>7637.54</td>\n",
       "      <td>12535.68</td>\n",
       "      <td>58.82</td>\n",
       "      <td>2.40</td>\n",
       "      <td>13.731548</td>\n",
       "      <td>1.921365e+01</td>\n",
       "      <td>145944.07090</td>\n",
       "      <td>3918.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>3905.900</td>\n",
       "      <td>3921.600</td>\n",
       "      <td>3882.800</td>\n",
       "      <td>3918.900</td>\n",
       "      <td>2316.834940</td>\n",
       "      <td>9.055260e+06</td>\n",
       "      <td>11155</td>\n",
       "      <td>2818.46</td>\n",
       "      <td>6.7042</td>\n",
       "      <td>1316.30</td>\n",
       "      <td>7691.52</td>\n",
       "      <td>12641.36</td>\n",
       "      <td>59.94</td>\n",
       "      <td>2.40</td>\n",
       "      <td>27.948466</td>\n",
       "      <td>7.480000e-14</td>\n",
       "      <td>148260.90590</td>\n",
       "      <td>4025.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>3919.000</td>\n",
       "      <td>4037.300</td>\n",
       "      <td>3913.000</td>\n",
       "      <td>4025.800</td>\n",
       "      <td>4241.284352</td>\n",
       "      <td>1.693466e+07</td>\n",
       "      <td>8794</td>\n",
       "      <td>2805.37</td>\n",
       "      <td>6.7141</td>\n",
       "      <td>1309.70</td>\n",
       "      <td>7643.38</td>\n",
       "      <td>12591.72</td>\n",
       "      <td>59.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>76.241999</td>\n",
       "      <td>3.774635e+01</td>\n",
       "      <td>152502.19020</td>\n",
       "      <td>4013.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>4025.000</td>\n",
       "      <td>4025.000</td>\n",
       "      <td>3994.800</td>\n",
       "      <td>4013.800</td>\n",
       "      <td>2373.315090</td>\n",
       "      <td>9.520079e+06</td>\n",
       "      <td>8908</td>\n",
       "      <td>2815.44</td>\n",
       "      <td>6.7263</td>\n",
       "      <td>1295.15</td>\n",
       "      <td>7669.17</td>\n",
       "      <td>12632.58</td>\n",
       "      <td>59.30</td>\n",
       "      <td>2.41</td>\n",
       "      <td>68.509956</td>\n",
       "      <td>7.450024e+01</td>\n",
       "      <td>150128.87510</td>\n",
       "      <td>4088.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>4013.300</td>\n",
       "      <td>4100.000</td>\n",
       "      <td>4005.900</td>\n",
       "      <td>4088.900</td>\n",
       "      <td>4465.020015</td>\n",
       "      <td>1.814237e+07</td>\n",
       "      <td>10701</td>\n",
       "      <td>2834.40</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>1295.40</td>\n",
       "      <td>7729.32</td>\n",
       "      <td>12696.88</td>\n",
       "      <td>60.14</td>\n",
       "      <td>2.43</td>\n",
       "      <td>83.868040</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>154593.89510</td>\n",
       "      <td>4090.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>4088.400</td>\n",
       "      <td>4130.000</td>\n",
       "      <td>4040.000</td>\n",
       "      <td>4090.900</td>\n",
       "      <td>2493.453841</td>\n",
       "      <td>1.019285e+07</td>\n",
       "      <td>8671</td>\n",
       "      <td>2834.40</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>1295.40</td>\n",
       "      <td>7729.32</td>\n",
       "      <td>12696.88</td>\n",
       "      <td>60.14</td>\n",
       "      <td>2.43</td>\n",
       "      <td>84.176325</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>157087.34900</td>\n",
       "      <td>4095.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>4092.200</td>\n",
       "      <td>4100.000</td>\n",
       "      <td>4073.700</td>\n",
       "      <td>4095.200</td>\n",
       "      <td>947.568361</td>\n",
       "      <td>3.875028e+06</td>\n",
       "      <td>8228</td>\n",
       "      <td>2834.40</td>\n",
       "      <td>6.7335</td>\n",
       "      <td>1295.40</td>\n",
       "      <td>7729.32</td>\n",
       "      <td>12696.88</td>\n",
       "      <td>60.14</td>\n",
       "      <td>2.43</td>\n",
       "      <td>85.094931</td>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>158034.91730</td>\n",
       "      <td>4137.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Volume (BTC)  \\\n",
       "date                                                               \n",
       "2016-10-30   713.949   713.989   688.000   700.917   1750.780129   \n",
       "2016-10-31   700.079   708.899   685.700   698.919    982.631046   \n",
       "2016-11-01   698.790   739.488   697.910   733.744   2336.219977   \n",
       "2016-11-02   733.742   747.070   722.890   747.070   1902.393632   \n",
       "2016-11-03   747.070   750.500   673.407   692.179   4169.139506   \n",
       "2016-11-04   692.902   709.998   682.311   706.036   1904.910013   \n",
       "2016-11-05   706.036   714.471   698.478   706.478    630.687729   \n",
       "2016-11-06   706.460   721.458   702.879   715.497    638.744099   \n",
       "2016-11-07   715.000   715.413   700.160   707.345    917.369981   \n",
       "2016-11-08   704.142   718.300   704.142   712.500    967.349179   \n",
       "2016-11-09   712.690   745.492   708.682   722.400   2759.317961   \n",
       "2016-11-10   722.400   724.696   706.000   714.000   1231.380014   \n",
       "2016-11-11   713.503   718.990   712.002   717.750    719.858395   \n",
       "2016-11-12   717.740   719.100   701.348   704.000    722.713797   \n",
       "2016-11-13   704.399   706.794   685.327   701.670   1072.468637   \n",
       "2016-11-14   702.700   709.000   697.791   706.994    785.108494   \n",
       "2016-11-15   706.999   717.000   706.081   711.310    848.937713   \n",
       "2016-11-16   710.870   744.800   709.000   741.603   2465.339553   \n",
       "2016-11-17   741.875   760.814   733.300   739.100   3453.915169   \n",
       "2016-11-18   739.088   754.000   733.010   752.232   1730.442485   \n",
       "2016-11-19   752.232   763.110   735.000   751.990   1420.026691   \n",
       "2016-11-20   751.990   757.000   713.000   729.677   1413.783356   \n",
       "2016-11-21   731.000   741.000   728.330   738.000    545.090343   \n",
       "2016-11-22   737.999   752.614   733.830   749.760   1806.774640   \n",
       "2016-11-23   749.760   750.000   731.060   741.980   1283.534229   \n",
       "2016-11-24   742.489   745.110   730.000   738.440    867.198395   \n",
       "2016-11-25   740.459   743.000   730.345   742.140    985.011973   \n",
       "2016-11-26   742.479   743.300   726.000   733.170    678.236092   \n",
       "2016-11-27   733.260   739.000   727.200   729.930    637.789927   \n",
       "2016-11-28   729.885   736.071   728.100   734.170    819.240542   \n",
       "...              ...       ...       ...       ...           ...   \n",
       "2019-03-02  3804.600  3818.500  3754.300  3809.900   1766.652670   \n",
       "2019-03-03  3810.300  3822.400  3755.400  3785.700   2204.155819   \n",
       "2019-03-04  3785.700  3805.200  3670.900  3700.100   4484.164154   \n",
       "2019-03-05  3700.000  3875.100  3690.600  3844.900   4451.450719   \n",
       "2019-03-06  3845.000  3895.700  3805.000  3851.700   3444.455415   \n",
       "2019-03-07  3851.700  3891.400  3829.900  3854.000   4079.085996   \n",
       "2019-03-08  3854.000  3910.000  3760.300  3844.300   4658.364326   \n",
       "2019-03-09  3840.500  3947.000  3836.700  3919.200   3290.699350   \n",
       "2019-03-10  3916.100  3916.300  3863.700  3897.900   1449.901130   \n",
       "2019-03-11  3899.800  3911.200  3815.500  3850.700   3229.684251   \n",
       "2019-03-12  3852.000  3879.600  3800.400  3863.100   3277.170766   \n",
       "2019-03-13  3864.400  3873.500  3826.200  3853.400   3013.369886   \n",
       "2019-03-14  3851.300  3907.000  3781.600  3853.900   3598.009049   \n",
       "2019-03-15  3853.900  3909.800  3846.800  3900.200   2890.546813   \n",
       "2019-03-16  3900.100  4042.500  3899.700  3989.200   4433.965270   \n",
       "2019-03-17  3989.500  3989.600  3937.400  3965.000   1470.329979   \n",
       "2019-03-18  3965.100  4019.900  3935.200  3969.600   3731.057533   \n",
       "2019-03-19  3969.600  4010.000  3950.000  4000.000   3209.596846   \n",
       "2019-03-20  3999.900  4048.800  3963.400  4032.100   3934.607074   \n",
       "2019-03-21  4030.900  4064.400  3871.300  3972.400   4867.144416   \n",
       "2019-03-22  3972.500  3999.000  3960.000  3982.100   2481.959080   \n",
       "2019-03-23  3982.100  4000.400  3961.000  3982.500   1616.990068   \n",
       "2019-03-24  3982.400  3982.400  3944.400  3969.300   2067.316000   \n",
       "2019-03-25  3968.600  3976.600  3855.500  3905.900   4822.948801   \n",
       "2019-03-26  3905.900  3921.600  3882.800  3918.900   2316.834940   \n",
       "2019-03-27  3919.000  4037.300  3913.000  4025.800   4241.284352   \n",
       "2019-03-28  4025.000  4025.000  3994.800  4013.800   2373.315090   \n",
       "2019-03-29  4013.300  4100.000  4005.900  4088.900   4465.020015   \n",
       "2019-03-30  4088.400  4130.000  4040.000  4090.900   2493.453841   \n",
       "2019-03-31  4092.200  4100.000  4073.700  4095.200    947.568361   \n",
       "\n",
       "            Volume (Currency)  index   biaopu  exchange     gold   nasdaq  \\\n",
       "date                                                                        \n",
       "2016-10-30       1.228539e+06   5565  2126.41    6.7858  1273.00  5190.10   \n",
       "2016-10-31       6.878437e+05   4467  2126.15    6.7641  1272.00  5189.14   \n",
       "2016-11-01       1.702318e+06   4587  2111.72    6.7734  1288.45  5153.58   \n",
       "2016-11-02       1.390995e+06   6337  2097.94    6.7562  1303.75  5105.57   \n",
       "2016-11-03       2.969056e+06   6845  2088.66    6.7491  1301.00  5058.41   \n",
       "2016-11-04       1.335331e+06   6811  2085.18    6.7514  1302.80  5046.37   \n",
       "2016-11-05       4.452695e+05   6048  2085.18    6.7514  1302.80  5046.37   \n",
       "2016-11-06       4.555510e+05   4572  2085.18    6.7514  1302.80  5046.37   \n",
       "2016-11-07       6.484827e+05   5370  2131.52    6.7725  1283.05  5166.17   \n",
       "2016-11-08       6.879393e+05   5380  2139.56    6.7817  1282.35  5193.49   \n",
       "2016-11-09       2.012925e+06   5240  2163.26    6.7832  1281.40  5251.07   \n",
       "2016-11-10       8.802056e+05   4749  2167.48    6.7885  1267.50  5208.80   \n",
       "2016-11-11       5.152334e+05   3735  2164.45    6.8115  1236.45  5237.11   \n",
       "2016-11-12       5.113135e+05   4153  2164.45    6.8115  1236.45  5237.11   \n",
       "2016-11-13       7.445580e+05   4225  2164.45    6.8115  1236.45  5237.11   \n",
       "2016-11-14       5.512836e+05   7171  2164.20    6.8291  1213.60  5218.40   \n",
       "2016-11-15       6.048656e+05   5907  2180.39    6.8495  1226.95  5275.62   \n",
       "2016-11-16       1.804220e+06   5995  2176.94    6.8592  1229.20  5294.58   \n",
       "2016-11-17       2.576195e+06   7811  2187.12    6.8692  1226.75  5333.97   \n",
       "2016-11-18       1.292808e+06   7623  2181.90    6.8796  1211.00  5321.51   \n",
       "2016-11-19       1.063850e+06   7221  2181.90    6.8796  1211.00  5321.51   \n",
       "2016-11-20       1.040871e+06   5976  2181.90    6.8796  1211.00  5321.51   \n",
       "2016-11-21       4.011288e+05   5694  2198.18    6.8985  1214.25  5368.86   \n",
       "2016-11-22       1.344081e+06   5312  2202.94    6.8779  1212.25  5386.35   \n",
       "2016-11-23       9.510890e+05   5380  2204.72    6.8904  1185.35  5380.69   \n",
       "2016-11-24       6.390440e+05   6227  2204.72    6.9085  1186.10  5380.69   \n",
       "2016-11-25       7.267828e+05   5634  2213.35    6.9168  1187.70  5398.92   \n",
       "2016-11-26       4.977535e+05   4480  2213.35    6.9168  1187.70  5398.92   \n",
       "2016-11-27       4.673402e+05   4324  2213.35    6.9168  1187.70  5398.92   \n",
       "2016-11-28       6.005968e+05   3797  2201.72    6.9042  1187.00  5368.81   \n",
       "...                       ...    ...      ...       ...      ...      ...   \n",
       "2019-03-02       6.714834e+06  11473  2803.69    6.6957  1311.95  7595.35   \n",
       "2019-03-03       8.362644e+06  11604  2803.69    6.6957  1311.95  7595.35   \n",
       "2019-03-04       1.665976e+07  13315  2792.81    6.7049  1285.40  7577.57   \n",
       "2019-03-05       1.690483e+07  13795  2789.65    6.6998  1283.80  7576.36   \n",
       "2019-03-06       1.323957e+07  11580  2771.45    6.7053  1285.85  7505.92   \n",
       "2019-03-07       1.575452e+07  12374  2748.93    6.7110  1285.30  7421.46   \n",
       "2019-03-08       1.799671e+07  10530  2743.07    6.7235  1296.75  7408.14   \n",
       "2019-03-09       1.281770e+07  12150  2743.07    6.7235  1296.75  7408.14   \n",
       "2019-03-10       5.639155e+06  11698  2743.07    6.7235  1296.75  7408.14   \n",
       "2019-03-11       1.245815e+07  12708  2783.30    6.7202  1292.75  7558.06   \n",
       "2019-03-12       1.261635e+07  12593  2791.52    6.7128  1297.05  7591.03   \n",
       "2019-03-13       1.161036e+07  12713  2810.92    6.7114  1306.95  7643.41   \n",
       "2019-03-14       1.384985e+07  12453  2808.48    6.7009  1295.55  7630.91   \n",
       "2019-03-15       1.122124e+07  11742  2822.48    6.7167  1303.50  7688.53   \n",
       "2019-03-16       1.763293e+07  10455  2822.48    6.7167  1303.50  7688.53   \n",
       "2019-03-17       5.825342e+06  10076  2822.48    6.7167  1303.50  7688.53   \n",
       "2019-03-18       1.481929e+07   8644  2832.94    6.7088  1305.60  7714.48   \n",
       "2019-03-19       1.278029e+07  11494  2832.57    6.7062  1307.70  7723.95   \n",
       "2019-03-20       1.574005e+07  13154  2824.23    6.7101  1303.70  7728.97   \n",
       "2019-03-21       1.941989e+07  12650  2854.88    6.6850  1309.60  7838.96   \n",
       "2019-03-22       9.879739e+06  13195  2800.71    6.6944  1311.30  7642.67   \n",
       "2019-03-23       6.436828e+06  14116  2800.71    6.6944  1311.30  7642.67   \n",
       "2019-03-24       8.197462e+06   9716  2800.71    6.6944  1311.30  7642.67   \n",
       "2019-03-25       1.892225e+07   9152  2798.36    6.7098  1319.55  7637.54   \n",
       "2019-03-26       9.055260e+06  11155  2818.46    6.7042  1316.30  7691.52   \n",
       "2019-03-27       1.693466e+07   8794  2805.37    6.7141  1309.70  7643.38   \n",
       "2019-03-28       9.520079e+06   8908  2815.44    6.7263  1295.15  7669.17   \n",
       "2019-03-29       1.814237e+07  10701  2834.40    6.7335  1295.40  7729.32   \n",
       "2019-03-30       1.019285e+07   8671  2834.40    6.7335  1295.40  7729.32   \n",
       "2019-03-31       3.875028e+06   8228  2834.40    6.7335  1295.40  7729.32   \n",
       "\n",
       "            niujiaosuo    oil  rate        RSI           mfi           obv  \\\n",
       "date                                                                         \n",
       "2016-10-30    10476.62  48.70  0.41  65.293669  6.197086e+01   59041.91285   \n",
       "2016-10-31    10481.89  46.86  0.31  60.736373  3.790429e+01   58059.28180   \n",
       "2016-11-01    10414.05  46.67  0.41  86.100549  4.691699e+01   60395.50178   \n",
       "2016-11-02    10334.50  45.34  0.41  89.860276  8.187062e+01   62297.89541   \n",
       "2016-11-03    10307.64  44.66  0.41  33.639305  5.129104e+01   58128.75590   \n",
       "2016-11-04    10289.35  44.07  0.41  46.349772  2.475579e+01   60033.66592   \n",
       "2016-11-05    10289.35  44.07  0.41  46.836969  9.442540e+00   60664.35365   \n",
       "2016-11-06    10289.35  44.07  0.41  58.399626  4.034681e+01   61303.09775   \n",
       "2016-11-07    10500.16  44.89  0.41  45.100424  5.812730e+01   60385.72776   \n",
       "2016-11-08    10530.56  44.98  0.41  54.852646  6.379801e+01   61353.07694   \n",
       "2016-11-09    10643.41  45.27  0.41  70.135108  8.056113e+01   64112.39490   \n",
       "2016-11-10    10683.41  44.66  0.41  49.017405  7.534608e+01   62881.01489   \n",
       "2016-11-11    10652.24  44.15  0.41  57.572124  7.409218e+01   63600.87328   \n",
       "2016-11-12    10652.24  44.15  0.41  29.940528  2.702717e+01   62878.15949   \n",
       "2016-11-13    10652.24  44.15  0.41  26.685107  2.903307e+01   61805.69085   \n",
       "2016-11-14    10679.77  43.94  0.41  46.589460  3.050396e+01   62590.79934   \n",
       "2016-11-15    10745.51  46.39  0.41  59.845750  6.072225e+01   63439.73706   \n",
       "2016-11-16    10699.43  46.10  0.41  88.886318  1.000000e+02   65905.07661   \n",
       "2016-11-17    10740.08  45.98  0.41  81.574308  1.000000e+02   62451.16144   \n",
       "2016-11-18    10709.51  46.36  0.41  88.815193  1.000000e+02   64181.60393   \n",
       "2016-11-19    10709.51  46.36  0.41  87.860775  1.000000e+02   62761.57724   \n",
       "2016-11-20    10709.51  46.36  0.41  35.338996  6.945106e+01   61347.79388   \n",
       "2016-11-21    10791.84  48.24  0.41  51.545566  5.858074e+01   61892.88422   \n",
       "2016-11-22    10820.18  48.03  0.41  68.355489  6.277115e+01   63699.65886   \n",
       "2016-11-23    10835.90  47.96  0.41  50.849627  6.475982e+01   62416.12463   \n",
       "2016-11-24    10835.90  47.96  0.41  43.283888  4.584366e+01   61548.92624   \n",
       "2016-11-25    10878.09  46.06  0.41  54.011489  3.137616e+01   62533.93821   \n",
       "2016-11-26    10878.09  46.06  0.41  32.000640  3.899949e+01   61855.70212   \n",
       "2016-11-27    10878.09  46.06  0.41  26.212894  4.298581e+01   61217.91219   \n",
       "2016-11-28    10808.63  47.08  0.41  45.545706  3.835577e+01   62037.15273   \n",
       "...                ...    ...   ...        ...           ...           ...   \n",
       "2019-03-02    12700.67  55.80  2.40  48.986830  5.596731e+01  133771.85570   \n",
       "2019-03-03    12700.67  55.80  2.40  33.935844  1.180000e-13  131567.69990   \n",
       "2019-03-04    12637.27  56.59  2.40  12.902485  9.380000e-14  127083.53570   \n",
       "2019-03-05    12624.47  56.56  2.40  66.145014  4.032638e+01  131534.98640   \n",
       "2019-03-06    12538.00  56.22  2.40  67.542661  6.438136e+01  134979.44190   \n",
       "2019-03-07    12443.43  56.66  2.40  68.208540  1.000000e+02  139058.52790   \n",
       "2019-03-08    12415.13  56.07  2.40  60.373135  6.186269e+01  134400.16350   \n",
       "2019-03-09    12415.13  56.07  2.40  82.996615  6.151215e+01  137690.86290   \n",
       "2019-03-10    12415.13  56.07  2.40  66.742564  3.530447e+01  136240.96180   \n",
       "2019-03-11    12561.26  56.79  2.40  40.426496  4.148348e+01  133011.27750   \n",
       "2019-03-12    12582.81  57.20  2.40  48.438076  1.090000e-13  136288.44830   \n",
       "2019-03-13    12674.26  58.59  2.40  41.836316  3.163915e+01  133275.07840   \n",
       "2019-03-14    12660.16  58.91  2.40  42.442859  3.049225e+01  136873.08740   \n",
       "2019-03-15    12715.77  58.82  2.40  76.492688  6.225858e+01  139763.63420   \n",
       "2019-03-16    12715.77  58.82  2.40  91.312098  6.758707e+01  144197.59950   \n",
       "2019-03-17    12715.77  58.82  2.40  72.635646  8.320077e+01  142727.26950   \n",
       "2019-03-18    12781.42  59.38  2.40  74.143534  8.477967e+01  146458.32710   \n",
       "2019-03-19    12760.79  59.29  2.40  83.277943  8.257820e+01  149667.92390   \n",
       "2019-03-20    12700.25  60.23  2.41  89.277596  1.000000e+02  153602.53100   \n",
       "2019-03-21    12782.54  59.98  2.41  44.618395  5.967682e+01  148735.38660   \n",
       "2019-03-22    12539.41  59.04  2.41  50.636541  5.706316e+01  151217.34570   \n",
       "2019-03-23    12539.41  59.04  2.41  50.966130  4.578709e+01  152834.33570   \n",
       "2019-03-24    12539.41  59.04  2.41  38.305976  6.655992e+01  150767.01970   \n",
       "2019-03-25    12535.68  58.82  2.40  13.731548  1.921365e+01  145944.07090   \n",
       "2019-03-26    12641.36  59.94  2.40  27.948466  7.480000e-14  148260.90590   \n",
       "2019-03-27    12591.72  59.41  2.41  76.241999  3.774635e+01  152502.19020   \n",
       "2019-03-28    12632.58  59.30  2.41  68.509956  7.450024e+01  150128.87510   \n",
       "2019-03-29    12696.88  60.14  2.43  83.868040  1.000000e+02  154593.89510   \n",
       "2019-03-30    12696.88  60.14  2.43  84.176325  1.000000e+02  157087.34900   \n",
       "2019-03-31    12696.88  60.14  2.43  85.094931  1.000000e+02  158034.91730   \n",
       "\n",
       "             predict  \n",
       "date                  \n",
       "2016-10-30   698.919  \n",
       "2016-10-31   733.744  \n",
       "2016-11-01   747.070  \n",
       "2016-11-02   692.179  \n",
       "2016-11-03   706.036  \n",
       "2016-11-04   706.478  \n",
       "2016-11-05   715.497  \n",
       "2016-11-06   707.345  \n",
       "2016-11-07   712.500  \n",
       "2016-11-08   722.400  \n",
       "2016-11-09   714.000  \n",
       "2016-11-10   717.750  \n",
       "2016-11-11   704.000  \n",
       "2016-11-12   701.670  \n",
       "2016-11-13   706.994  \n",
       "2016-11-14   711.310  \n",
       "2016-11-15   741.603  \n",
       "2016-11-16   739.100  \n",
       "2016-11-17   752.232  \n",
       "2016-11-18   751.990  \n",
       "2016-11-19   729.677  \n",
       "2016-11-20   738.000  \n",
       "2016-11-21   749.760  \n",
       "2016-11-22   741.980  \n",
       "2016-11-23   738.440  \n",
       "2016-11-24   742.140  \n",
       "2016-11-25   733.170  \n",
       "2016-11-26   729.930  \n",
       "2016-11-27   734.170  \n",
       "2016-11-28   732.000  \n",
       "...              ...  \n",
       "2019-03-02  3785.700  \n",
       "2019-03-03  3700.100  \n",
       "2019-03-04  3844.900  \n",
       "2019-03-05  3851.700  \n",
       "2019-03-06  3854.000  \n",
       "2019-03-07  3844.300  \n",
       "2019-03-08  3919.200  \n",
       "2019-03-09  3897.900  \n",
       "2019-03-10  3850.700  \n",
       "2019-03-11  3863.100  \n",
       "2019-03-12  3853.400  \n",
       "2019-03-13  3853.900  \n",
       "2019-03-14  3900.200  \n",
       "2019-03-15  3989.200  \n",
       "2019-03-16  3965.000  \n",
       "2019-03-17  3969.600  \n",
       "2019-03-18  4000.000  \n",
       "2019-03-19  4032.100  \n",
       "2019-03-20  3972.400  \n",
       "2019-03-21  3982.100  \n",
       "2019-03-22  3982.500  \n",
       "2019-03-23  3969.300  \n",
       "2019-03-24  3905.900  \n",
       "2019-03-25  3918.900  \n",
       "2019-03-26  4025.800  \n",
       "2019-03-27  4013.800  \n",
       "2019-03-28  4088.900  \n",
       "2019-03-29  4090.900  \n",
       "2019-03-30  4095.200  \n",
       "2019-03-31  4137.800  \n",
       "\n",
       "[883 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cryptocurrency-analysis]",
   "language": "python",
   "name": "conda-env-cryptocurrency-analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}